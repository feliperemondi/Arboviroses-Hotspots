# -*- coding: utf-8 -*-
"""Análise_de_Gi_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Pz2lUFzjB-CkfkfUjQ1yGvNs4alfvIw
"""

# SETUP INICIAL DO AMBIENTE
import os
from google.colab import drive
from IPython.display import clear_output # <--- Essa linha é obrigatória para o comando funcionar

# 1. Monta o Google Drive
# O parâmetro force_remount=True ajuda caso a conexão tenha caído
print(">>> Conectando ao Google Drive...")
drive.mount('/content/drive', force_remount=True)

# 2. Limpeza de Tela
# Remove as mensagens de montagem do Drive para deixar o log limpo
try:
    clear_output()
except NameError:
    print("Nota: clear_output não funcionou, mas o drive está montado.")

print(">>> [AMBIENTE] Google Drive conectado com sucesso.")
print(">>> [AMBIENTE] Saídas anteriores limpas. Iniciando rotina...")

"""# Processamento e harmonização"""

# IMPORTAÇÃO DE BIBLIOTECAS

# 1. Instalação de Dependências (Para ambiente Notebook/Colab)
# O dbfread não vem instalado por padrão no Python/Anaconda
try:
    import dbfread
except ImportError:
    print("Instalando biblioteca dbfread...")
    !pip install dbfread

# 2. Manipulação de Arquivos e Sistema
import os
import glob
import re
import shutil
from datetime import datetime

# 3. Processamento de Dados e Texto
import pandas as pd
import unicodedata
from dbfread import DBF # Importação específica da classe

# --- 1. SETUP E CONFIGURAÇÕES INICIAIS ---
print(">>> [1/5] Iniciando Setup e Leitura dos Arquivos...")

PATH_DBF = '/content/*.dbf'
PATH_CSV_MUN = 'ID_MN_RESI.CSV'

COLS_SINAN = [
    "NU_NOTIFIC", "NU_ANO", "DT_SIN_PRI", "SEM_PRI", "ID_MN_RESI",
    "NM_LOGRADO", "NU_NUMERO", "NM_COMPLEM", "CLASSI_FIN"
]

# Leitura da Tabela Auxiliar (Municípios)
# Importante: ID como string para evitar perda de zeros à esquerda
df_mun = pd.read_csv(PATH_CSV_MUN, sep=';', encoding='latin1', dtype={'ID_MN_RESI': str})
df_mun = df_mun[['ID_MN_RESI', 'Municipio']]

# Leitura e Consolidação dos DBFs
lista_dfs = []
for arq in glob.glob(PATH_DBF):
    try:
        # iter(DBF) é mais eficiente para memória
        df_temp = pd.DataFrame(iter(DBF(arq, load=False, encoding='cp1252')))
        # Filtra colunas existentes para evitar erro se o ano mudar schema
        cols_validas = [c for c in COLS_SINAN if c in df_temp.columns]
        lista_dfs.append(df_temp[cols_validas])
    except Exception as e:
        print(f"Erro ao ler {arq}: {e}")

if not lista_dfs:
    raise ValueError("ERRO CRÍTICO: Nenhum arquivo DBF encontrado.")

df = pd.concat(lista_dfs, ignore_index=True)
print(f"    Base carregada com {len(df)} registros.")


# --- 2. NORMALIZAÇÃO DOS CAMPOS ---

PREFIXOS_VIAS = [
    'RUA', 'R ', 'AV', 'AVENIDA', 'TRAVESSA', 'TV', 'ALAMEDA', 'AL',
    'PRACA', 'PC', 'BECO', 'RODOVIA', 'ROD', 'ESTRADA', 'EST',
    'VILA', 'VL', 'LARGO', 'JARDIM', 'JD', 'PARQUE', 'PQ', 'SERVIDAO'
]

def normalizar_texto(texto):
    """Remove acentos, caracteres especiais e normaliza para caixa alta."""
    if not isinstance(texto, str):
        return str(texto) if pd.notnull(texto) else ''

    # Remove acentos (NFKD decompõe caracteres)
    texto = unicodedata.normalize('NFKD', texto)
    texto_limpo = ''.join(c for c in texto if not unicodedata.combining(c))

    # Mantém apenas alfanuméricos e espaços
    texto_limpo = re.sub(r'[^a-zA-Z0-9\s]', ' ', texto_limpo)
    return texto_limpo.upper().strip()


# --- 3. QUALIFICAÇÃO E LIMPEZA (EXECUÇÃO) ---
print(">>> [2/5] Traduzindo Municípios e [3/5] Higienizando Dados...")

def qualificar_logradouro(texto):
    """Adiciona 'RUA' se o logradouro iniciar sem tipo definido."""
    if len(texto) < 2: return texto

    primeira_palavra = texto.split()[0]
    if primeira_palavra not in PREFIXOS_VIAS:
        return "RUA " + texto
    return texto

def resgatar_numero(row):
    """Tenta recuperar o número do final do logradouro ou do complemento."""
    # 1. Tenta pegar digitos no final da rua (Ex: RUA TESTE 123)
    match = re.search(r'(\d+)$', row['NM_LOGRADO'])
    if match: return match.group(1)

    # 2. Tenta pegar do complemento se for curto e numérico
    if row['NM_COMPLEM'].isdigit() and len(row['NM_COMPLEM']) < 6:
        return row['NM_COMPLEM']
    return ''

# A. Tradução (Vincular Nome do Município pelo Código)
df['ID_MN_RESI'] = df['ID_MN_RESI'].astype(str).str.replace(r'\.0$', '', regex=True).str.strip()
df = pd.merge(df, df_mun, on='ID_MN_RESI', how='left')

# B. Normalização Geral (Texto)
cols_texto = ['NM_LOGRADO', 'NM_COMPLEM', 'Municipio']
for col in cols_texto:
    if col in df.columns:
        df[col] = df[col].apply(normalizar_texto)

# C. Qualificação (Lógica específica de endereço)
df['NM_LOGRADO'] = df['NM_LOGRADO'].apply(qualificar_logradouro)

# D. Limpeza Numérica
df['NU_NUMERO'] = df['NU_NUMERO'].fillna('').astype(str).str.replace(r'\.0$', '', regex=True).str.strip()
df['CLASSI_FIN'] = df['CLASSI_FIN'].fillna('').astype(str)

# E. Recuperação de Números Perdidos (Vazio)
mask_sem_num = df['NU_NUMERO'] == ''
if mask_sem_num.any():
    df.loc[mask_sem_num, 'NU_NUMERO'] = df[mask_sem_num].apply(resgatar_numero, axis=1)


# --- 4. CONSTRUÇÃO DA VARIÁVEL ENDEREÇO ---
print(">>> [4/5] Construindo variável 'END' otimizada...")

# Critério mínimo: Rua > 3 chars e Número existente
mask_end_valido = (df['NM_LOGRADO'].str.len() > 3) & (df['NU_NUMERO'].str.len() > 0)

df['END'] = '' # Inicializa vazio
df.loc[mask_end_valido, 'END'] = (
    df.loc[mask_end_valido, 'NM_LOGRADO'] + ", " +
    df.loc[mask_end_valido, 'NU_NUMERO'] + ", " +
    df.loc[mask_end_valido, 'Municipio'] + " - PR, BRASIL"
)

# Metadados do processo
df['DT_PROCESSAMENTO'] = datetime.now().strftime('%d%m%Y')


# --- RELATÓRIO E ARQUIVO DE VERIFICACÃO ---

print(">>> [5/5] Finalizando e Exportando...")
print(">>> [5/5] Gerando Relatório de Qualidade...")

# Máscaras de Classificação
mask_descartado = df['CLASSI_FIN'] == '5'
mask_provavel = df['CLASSI_FIN'] != '5'
mask_sem_endereco = df['END'] == ''

# Agrupamento por Município
# 1. Total Geral
s_total = df.groupby('Municipio').size().rename('Total_Notificacoes')

# 2. Casos Descartados (Não são dengue)
s_descartados = df[mask_descartado].groupby('Municipio').size().rename('Casos_Descartados')

# 3. Casos Prováveis (Universo de Interesse)
s_provaveis = df[mask_provavel].groupby('Municipio').size().rename('Casos_Provaveis')

# 4. Falha de Qualidade (Prováveis SEM endereço completo)
s_falha_end = df[mask_provavel & mask_sem_endereco].groupby('Municipio').size().rename('Provaveis_Sem_Endereço')

# Consolidação da Auditoria
df_auditoria = pd.concat([s_total, s_descartados, s_provaveis, s_falha_end], axis=1).fillna(0).astype(int)

# Cálculo da Taxa de Incompletude (%)
# (Dos prováveis, quantos por cento não têm endereço?)
df_auditoria['%_Incompletude'] = (df_auditoria['Provaveis_Sem_Endereço'] / df_auditoria['Casos_Provaveis'] * 100).round(1)

# Ordenação e Visualização
colunas_ordem = ['Total_Notificacoes', 'Casos_Descartados', 'Casos_Provaveis', 'Provaveis_Sem_Endereço', '%_Incompletude']
print("\n--- Auditoria de Qualidade do Endereço (Top 5 Carga) ---")
print(df_auditoria[colunas_ordem].sort_values('Total_Notificacoes', ascending=False).head().to_string())

# Exportação
PASTA_VERIFICACAO = "verificacao"
os.makedirs(PASTA_VERIFICACAO, exist_ok=True)

ARQUIVO_SAIDA = f"verificacao_etapa1_{datetime.now().strftime('%Y%m%d')}.csv"
CAMINHO_COMPLETO = os.path.join(PASTA_VERIFICACAO, ARQUIVO_SAIDA)

df.to_csv(CAMINHO_COMPLETO, index=False, sep=';', encoding='utf-8-sig')

print(f"\nArquivo Etapa 1 gerado com sucesso em:\n{CAMINHO_COMPLETO}")

"""# Georreferenciamento (ERSI ARCGIS)"""

# IMPORTAÇÃO DE BIBLIOTECAS

# 1. Sistema, Rede e Controle de Fluxo
import os
import glob
import time
import json
import requests
from datetime import datetime
from tqdm import tqdm  # Mantido: Gera barra de progresso visual no terminal

# 2. Processamento de Dados e Texto
import pandas as pd
import numpy as np
import unicodedata

# 3. Inteligência Espacial
import geopandas as gpd

# --- 1. SETUP INICIAL DO DF ---

if 'df' in globals() and isinstance(df, pd.DataFrame):
    print("Status: DataFrame encontrado na memória. Prosseguindo...")
else:
    print("Status: DataFrame não encontrado na memória. Buscando arquivo da Etapa 1 no disco...")

    # 1. Tenta na pasta 'verificacao' (Local padrão)
    padrao_pasta = os.path.join("verificacao", "verificacao_etapa1_*.csv")
    arquivos = sorted(glob.glob(padrao_pasta), reverse=True)

    # 2. Se falhar, tenta na raiz (Plano B)
    if not arquivos:
        padrao_raiz = "verificacao_etapa1_*.csv"
        arquivos = sorted(glob.glob(padrao_raiz), reverse=True)

    if arquivos:
        arquivo_alvo = arquivos[0]
        print(f"Carregando: {os.path.basename(arquivo_alvo)}")
        # sep=';' e dtype=str são vitais para não quebrar zeros à esquerda
        df = pd.read_csv(arquivo_alvo, sep=';', dtype=str)
    else:
        raise FileNotFoundError("ERRO CRÍTICO: Execute o Bloco 1 primeiro. Nenhum dado encontrado.")

# --- 2. CONFIGURAÇÃO DA API DO ARCGIS ---

# FUNÇÃO DE AUTENTICAÇÃO
def obter_chave_arcgis():
    # Lista de locais para procurar, em ordem de prioridade
    caminhos_busca = [
        "/content/drive/MyDrive/AUTOMAÇÃO PYTHON/Arquivos de apoio/ArcGis_API_Key.txt", # 1. Drive
        "ArcGis_API_Key.txt"                                                            # 2. Local (/content)
    ]

    for caminho in caminhos_busca:
        if os.path.exists(caminho):
            try:
                with open(caminho, "r", encoding='utf-8') as f:
                    chave = f.read().strip()
                    # Validação simples
                    if len(chave) > 20:
                        print(f"    > [AUTH] Chave encontrada em: '{caminho}'")
                        return chave
            except Exception as e:
                print(f"    [ERRO] Arquivo encontrado em '{caminho}', mas falha ao ler: {e}")
                # Se falhar a leitura, o loop continua para tentar o próximo caminho (se houver)
                continue

    # Se sair do loop sem retornar nada, é porque não achou
    return None

# EXECUÇÃO DA CONFIGURAÇÃO
ARCGIS_API_KEY = None
ARCGIS_URL = "https://geocode-api.arcgis.com/arcgis/rest/services/World/GeocodeServer/geocodeAddresses"

chave_obtida = obter_chave_arcgis()

if chave_obtida:
    ARCGIS_API_KEY = chave_obtida
    print("    [OK] API Configurada e pronta para uso.")
else:
    print("\n" + "!"*60)
    print("[ALERTA] A Chave da API não foi encontrada.")
    print("Verifique se o arquivo 'ArcGis_API_Key.txt' está na pasta '/content/'")
    print("OU no diretório do Drive: '/content/drive/MyDrive/AUTOMAÇÃO PYTHON/Arquivos de apoio/'")
    print("!"*60 + "\n")

# DEFINIÇÃO DA FUNÇÃO DE GEOCODIFICAÇÃO EM LOTE
BATCH_SIZE = 50
PAUSA_ENTRE_LOTES = 0.5
TIMEOUT_API = 60

def geocode_batch_arcgis(lista_enderecos):
    """
    Envia lote de endereços para a API do ArcGIS e retorna coordenadas + metadados.
    """
    resultados = {}
    records = []

    # Se não tem chave, nem tenta (evita erro 403)
    if not ARCGIS_API_KEY:
        return {}

    # Monta estrutura da requisição
    for i, end in enumerate(lista_enderecos):
        # OBJECTID é crucial para mapear a volta
        records.append({"attributes": {"OBJECTID": i, "SingleLine": end}})

    payload = {
        "addresses": json.dumps({"records": records}),
        "f": "json",
        "token": ARCGIS_API_KEY,
        "sourceCountry": "BRA",
        "outFields": "City,Addr_type,Match_addr,Score"
    }

    try:
        r = requests.post(ARCGIS_URL, data=payload, timeout=TIMEOUT_API)

        if r.status_code == 200:
            data = r.json()
            if 'locations' in data:
                for loc in data['locations']:
                    # Mapeia o resultado de volta ao endereço original usando o ID
                    oid = loc['attributes']['ResultID']
                    end_original = lista_enderecos[oid]
                    attrs = loc['attributes']

                    resultados[end_original] = {
                        'LATITUDE': loc['location']['y'],
                        'LONGITUDE': loc['location']['x'],
                        'GEO_SCORE': attrs.get('Score'),
                        'GEO_MATCH_ADDR': attrs.get('Match_addr'),
                        'GEO_ADDR_TYPE': attrs.get('Addr_type'),
                        'GEO_CITY': attrs.get('City', '')
                    }
            elif 'error' in data:
                 print(f"Erro lógico retornado pela API: {data['error']}")
        else:
            print(f"Falha na conexão HTTP: {r.status_code}")

    except Exception as e:
        print(f"Exceção crítica no lote: {e}")

    return resultados

# --- 3. SELEÇÃO DOS CASOS PARA GEORREFERENCIAMENTO ---

if 'df' not in globals():
    raise RuntimeError("O DataFrame 'df' não está carregado. Execute a Etapa 1.")

# Filtro: Casos Prováveis e com endereço
mask_aptos = (df['CLASSI_FIN'] != '5') & (df['END'].notnull()) & (df['END'] != '')
df_aptos = df[mask_aptos].copy()

# Criação da Tabela de Frequência
relatorio_geocoding = df_aptos.groupby('Municipio').agg(
    Casos_Totais=('END', 'size'),
    Enderecos_Unicos=('END', 'nunique')
).reset_index()

relatorio_geocoding['Economia_API'] = relatorio_geocoding['Casos_Totais'] - relatorio_geocoding['Enderecos_Unicos']

total_casos = relatorio_geocoding['Casos_Totais'].sum()
total_api = relatorio_geocoding['Enderecos_Unicos'].sum()
economia_perc = (1 - total_api/total_casos) * 100 if total_casos > 0 else 0

print("-" * 40)
print(f"DIAGNÓSTICO PRÉ-GEOCODIFICAÇÃO")
print("-" * 40)
print(f"Total de Casos a Mapear: {total_casos}")
print(f"Total de Chamadas à API (Custo): {total_api}")
print(f"Endereços Repetidos (Economia): {total_casos - total_api} ({economia_perc:.1f}% de otimização)")
print("-" * 40)

# --- 4. CONFIGURAÇÃO DE CACHE ---
PASTA_CACHE = "cache"
os.makedirs(PASTA_CACHE, exist_ok=True)
ARQUIVO_CACHE = os.path.join(PASTA_CACHE, "geocache_api_master.pkl")

print(f"Cache definido em: {ARQUIVO_CACHE}")

cache_memoria = {}
if os.path.exists(ARQUIVO_CACHE):
    try:
        df_cache = pd.read_pickle(ARQUIVO_CACHE)
        # Garante que não haja duplicatas no índice ao carregar
        df_cache = df_cache[~df_cache.index.duplicated(keep='last')]
        if 'END' in df_cache.columns:
             df_cache = df_cache.set_index('END')

        cache_memoria = df_cache.to_dict(orient='index')
        print(f"Recuperados {len(cache_memoria)} endereços do cache.")
    except Exception as e:
        print(f"Erro ao ler cache ({e}). Iniciando vazio.")
else:
    print("Nenhum cache encontrado. Iniciando processamento do zero.")

# --- 5. FILA DE PROCESSAMENTO (DEDUPLICACAO) ---
mask_validos = (df['END'] != '') & (df['CLASSI_FIN'] != '5')
todos_ends_unicos = df.loc[mask_validos, 'END'].unique()

fila_pendente = [end for end in todos_ends_unicos if end not in cache_memoria]

print(f"\nSTATUS DA FILA:")
print(f"Total Endereços Únicos Válidos: {len(todos_ends_unicos)}")
print(f"Já no Cache: {len(cache_memoria)}")
print(f"A processar agora: {len(fila_pendente)}")

# --- 6. GEORREFERENCIAMENTO - LOOP DE EXECUCAO ---
resultados_acumulados = cache_memoria.copy()

if fila_pendente:
    for i in tqdm(range(0, len(fila_pendente), BATCH_SIZE), desc="Consultando API"):
        lote = fila_pendente[i : i + BATCH_SIZE]
        novos = geocode_batch_arcgis(lote)
        resultados_acumulados.update(novos)

        # Salvamento incremental do Cache (Pickle)
        df_temp = pd.DataFrame.from_dict(resultados_acumulados, orient='index')
        df_temp.index.name = 'END'
        df_temp.reset_index(inplace=True)
        df_temp.to_pickle(ARQUIVO_CACHE)

        time.sleep(PAUSA_ENTRE_LOTES)

# --- 7. COMPILAÇÃO DO ARQUIVO COM ENDEREÇOS GEORREFERENCIADOS ---
print("\nCompilando...")

df_geo_ref = pd.DataFrame.from_dict(resultados_acumulados, orient='index')
cols_map = {
    'LATITUDE': 'Y', 'LONGITUDE': 'X',
    'GEO_MATCH_ADDR': 'GEO_MATCH_ADDR', 'GEO_SCORE': 'GEO_SCORE',
    'GEO_CITY': 'GEO_CITY', 'GEO_ADDR_TYPE': 'GEO_ADDR_TYPE'
}
df_geo_ref = df_geo_ref.rename(columns=cols_map)
df_geo_ref.index.name = 'END'
df_geo_ref.reset_index(inplace=True)

# --- 8. CONVERSÃO MATEMÁTICA DAS COORDENADAS PARA SIRGAS 2000 ---
if not df_geo_ref.empty:
    print(" > Iniciando reprojeção de WGS84 (GPS) para SIRGAS 2000 (BR)...")

    # Cria geometria temporária com o CRS original da API (EPSG:4326)
    gdf_temp = gpd.GeoDataFrame(
        df_geo_ref,
        geometry=gpd.points_from_xy(df_geo_ref.X, df_geo_ref.Y),
        crs="EPSG:4326"
    )

    # Converte para SIRGAS 2000 (EPSG:4674)
    gdf_sirgas = gdf_temp.to_crs("EPSG:4674")

    # Atualiza as colunas X e Y com os novos valores
    df_geo_ref['X'] = gdf_sirgas.geometry.x
    df_geo_ref['Y'] = gdf_sirgas.geometry.y

    # Marca a fonte como convertida
    df_geo_ref['GEO_SOURCE'] = "ArcGIS API (SIRGAS 2000)"
else:
    df_geo_ref['GEO_SOURCE'] = None

# Limpa colunas antigas do DF principal
cols_geo = list(cols_map.values()) + ['GEO_SOURCE']
df_limpo = df.drop(columns=[c for c in cols_geo if c in df.columns], errors='ignore')

# Merge Final
df_final = pd.merge(df_limpo, df_geo_ref, on='END', how='left')
mask_limpar = (df_final['CLASSI_FIN'] == '5') | (df_final['END'].isna()) | (df_final['END'] == '')
df_final.loc[mask_limpar, cols_geo] = np.nan


# --- RELATÓRIO E ARQUIVO DE VERIFICACÃO ---

mask_provavel = df_final['CLASSI_FIN'] != '5'
total_prov = mask_provavel.sum()
sucesso = df_final.loc[mask_provavel, 'Y'].notnull().sum()
erro_descartado = df_final.loc[(df_final['CLASSI_FIN'] == '5'), 'Y'].notnull().sum()

print("-" * 40)
print(f"RELATORIO DO MERGE (ETAPA 2)")
print(f"Total Casos Prováveis: {total_prov}")
print(f"Com Coordenadas SIRGAS 2000: {sucesso}")

if total_prov > 0:
    print(f"Taxa de Atribuição (Prováveis): {(sucesso/total_prov)*100:.2f}%")

print(f"Check de Integridade (Descartados com Geo): {erro_descartado} (Deve ser 0)")
print("-" * 40)

# Exportação Bruta
PASTA_VERIFICACAO = "verificacao"
os.makedirs(PASTA_VERIFICACAO, exist_ok=True)
nome_arquivo_verificacao = f"verificacao_etapa2_bruta_{datetime.now().strftime('%Y%m%d')}.csv"
caminho_verificacao = os.path.join(PASTA_VERIFICACAO, nome_arquivo_verificacao)

df_final.to_csv(caminho_verificacao, sep=';', index=False, encoding='utf-8-sig')
print(f"Arquivo bruto gerado em: {caminho_verificacao}")

# --- 10. HIGIENIZAÇÃO DE CASOS GEORREFERENCIADOS ---
LIMITE_REPETICOES = 10
TIPOS_RUINS = ['Locality', 'Municipality', 'Postal', 'Admin']

def normalizar(texto):
    if not isinstance(texto, str): return ''
    return ''.join(c for c in unicodedata.normalize('NFKD', texto)
                   if not unicodedata.combining(c)).upper().strip()

mask_com_geo = df_final['Y'].notnull()
df_final['GEO_STATUS'] = np.where(mask_com_geo, 'END_VALIDO', None)

# Validação A: Consistência Municipal
print("1. Verificando consistência municipal...")
df_final['CHECK_MUN_ORIG'] = df_final['Municipio'].apply(normalizar)
df_final['CHECK_MUN_GEO'] = df_final['GEO_CITY'].fillna('').apply(normalizar)

mask_cidade_errada = (
    mask_com_geo &
    (df_final['CHECK_MUN_GEO'] != '') &
    (df_final['CHECK_MUN_ORIG'] != df_final['CHECK_MUN_GEO'])
)
df_final.loc[mask_cidade_errada, 'GEO_STATUS'] = "ERRO: Cidade Divergente"

# Validação B: Precisão
print("2. Verificando precisão do endereço...")
mask_centroide = mask_com_geo & df_final['GEO_ADDR_TYPE'].isin(TIPOS_RUINS)
df_final.loc[mask_centroide, 'GEO_STATUS'] = "ERRO: Baixa Precisão"

# Validação C: Stack
print("3. Verificando empilhamento excessivo...")
coord_counts = df_final[mask_com_geo].groupby(['X', 'Y']).size().reset_index(name='FREQ_ESPACIAL')

df_final = df_final.drop(columns=['FREQ_ESPACIAL'], errors='ignore')
df_final = pd.merge(df_final, coord_counts, on=['X', 'Y'], how='left')

mask_stack = df_final['FREQ_ESPACIAL'] > LIMITE_REPETICOES
df_final.loc[mask_stack, 'GEO_STATUS'] = f"ERRO: Stack > {LIMITE_REPETICOES}"

# Limpeza de colunas auxiliares
df_final.drop(columns=['CHECK_MUN_ORIG', 'CHECK_MUN_GEO', 'FREQ_ESPACIAL'], inplace=True, errors='ignore')


# --- RELATÓRIO E ARQUIVO DE VERIFICACÃO ---

mask_prov = df_final['CLASSI_FIN'] != '5'
total_prov = mask_prov.sum()  # Universo Total de Interesse

# 1. Quantos tentamos geocodificar
total_geo_tentativa = df_final.loc[mask_prov, 'Y'].notnull().sum()

# 2. Quantos sobreviveram à higienização
resumo_status = df_final.loc[mask_prov, 'GEO_STATUS'].value_counts()
qtd_validos = resumo_status.get('END_VALIDO', 0)

# 3. Detalhamento das perdas
qtd_cidade = resumo_status.get('ERRO: Cidade Divergente', 0)
qtd_precisao = resumo_status.get('ERRO: Baixa Precisão', 0)
qtd_stack = resumo_status.get(f"ERRO: Stack > {LIMITE_REPETICOES}", 0)
total_rejeitados = qtd_cidade + qtd_precisao + qtd_stack

# 4. Aproveitamento Final
taxa_aproveitamento_total = (qtd_validos / total_prov * 100) if total_prov > 0 else 0

print("\n" + "="*50)
print("RELATÓRIO FINAL DE PERFORMANCE (ETAPA 2)")
print("="*50)
print(f"UNIVERSO (Total Casos Prováveis): {total_prov}")
print("-" * 50)
print(f"Perdas na Origem (Sem Endereço): {total_prov - total_geo_tentativa}")
print(f"Perdas na Higienização (Qualidade): {total_rejeitados}")
print(f"  > Cidade Divergente : {qtd_cidade}")
print(f"  > Baixa Precisão    : {qtd_precisao}")
print(f"  > Empilhamento      : {qtd_stack}")
print("-" * 50)
print(f"SUCESSO FINAL (Geo Válido)     : {qtd_validos}")
print(f"TAXA DE APROVEITAMENTO TOTAL   : {taxa_aproveitamento_total:.2f}%")
print("="*50)

# --- EXPORTAÇÃO 1: CSV DE SISTEMA (PASTA VERIFICACAO) ---
# Mantém o padrão para o Bloco 3 ler
ARQUIVO_CSV = os.path.join(PASTA_VERIFICACAO, f"verificacao_etapa2_higienizada_{datetime.now().strftime('%Y%m%d')}.csv")

print(f"\n1. Gerando CSV de sistema...")
df_final.to_csv(ARQUIVO_CSV, sep=';', index=False, encoding='utf-8-sig')
print(f"   > Salvo em: {ARQUIVO_CSV}")

# --- EXPORTAÇÃO 2: EXCEL GERENCIAL (PASTA RAIZ / CONTENT) ---
# Nome solicitado: georreferenciamento_completo_"DATA"
nome_excel = f"georreferenciamento_completo_{datetime.now().strftime('%Y%m%d')}.xlsx"
# Salva diretamente na raiz (sem os.path.join com pasta)
ARQUIVO_EXCEL = nome_excel

print(f"\n2. Gerando Excel gerencial final...")
try:
    with pd.ExcelWriter(ARQUIVO_EXCEL, engine='openpyxl') as writer:
        # ABA 1: Base Completa (Auditável)
        df_final.to_excel(writer, sheet_name='BASE_COMPLETA', index=False)

        # ABAS POR ANO: Apenas Higienizados
        df_validos = df_final[df_final['GEO_STATUS'] == 'END_VALIDO'].copy()

        if 'NU_ANO' in df_validos.columns:
            anos_disponiveis = sorted(df_validos['NU_ANO'].dropna().unique())
            for ano in anos_disponiveis:
                nome_aba = f"GEO_{str(ano)}"
                df_ano = df_validos[df_validos['NU_ANO'] == ano]
                if not df_ano.empty:
                    df_ano.to_excel(writer, sheet_name=nome_aba, index=False)
                    print(f"   > Aba '{nome_aba}' gerada.")

    print(f"   > Salvo na raiz: {ARQUIVO_EXCEL}")

except Exception as e:
    print(f"   [AVISO] Erro no Excel ({e}). O CSV acima garante os dados.")

print("\nProcesso da Etapa 2 finalizado.")

"""# Vinculação aos arquivos territoriais"""

# IMPORTAÇÃO DE BIBLIOTECAS

# 1. Sistema Operacional e Arquivos
import os
import glob
import zipfile
from datetime import datetime

# 2. Processamento de Dados
import pandas as pd

# 3. Inteligência Espacial e Mapas
import geopandas as gpd
import fiona
import folium

# 1. SETUP E CONFIGURAÇÕES INICIAIS DO DF

df_dados = None

# Verificação na memória do notebook
if 'df_final' in globals() and isinstance(df_final, pd.DataFrame):
    print("    [FONTE] Usando 'df_final' da memória RAM.")
    df_dados = df_final.copy()
elif 'df' in globals() and isinstance(df, pd.DataFrame):
    print("    [FONTE] Usando 'df' da memória RAM.")
    df_dados = df.copy()

# Se não disponível, carrega o CSV exportado no bloco 2 (após upload)
if df_dados is None:
    padrao_dados = os.path.join("verificacao_etapa2_higienizada_*.csv")
    arquivos_dados = sorted(glob.glob(padrao_dados), reverse=True)

    if arquivos_dados:
        arquivo_alvo = arquivos_dados[0]
        print(f"    [ARQUIVO] Carregando CSV: {os.path.basename(arquivo_alvo)}")
        df_dados = pd.read_csv(arquivo_alvo, sep=';', dtype=str)
    else:
        raise FileNotFoundError("ERRO CRÍTICO: Nenhum dado encontrado (nem RAM, nem CSV). Execute o Bloco 2.")

# Guarda o total original apenas para estatística
total_bruto = len(df_dados)

# Converte coordenadas para numérico
df_dados['X'] = pd.to_numeric(df_dados['X'], errors='coerce')
df_dados['Y'] = pd.to_numeric(df_dados['Y'], errors='coerce')

# Relatório
qtd_provaveis = len(df_dados[df_dados['CLASSI_FIN'].astype(str) != '5'])
qtd_validos = len(df_dados[df_dados['GEO_STATUS'] == 'END_VALIDO'])

print("-" * 40)
print(f"RESUMO DA PREPARAÇÃO")
print("-" * 40)
print(f"Total de notificações: {total_bruto}")
print(f"Total de casos prováveis: {qtd_provaveis}")
print(f"Total com END_VALIDO: {qtd_validos}")
print("-" * 40)

# Exibe amostra dos dados já filtrados
display(df_dados.head())

# --- 2. IMPORTA E CONVERTE O MAPA ---

# Configura drivers para ler KML/KMZ
fiona.drvsupport.supported_drivers['KML'] = 'rw'
fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'

# Cria pasta para arquivos temporários (evita poluir a raiz)
pasta_cache = "cache"
os.makedirs(pasta_cache, exist_ok=True)

lista_mapas = glob.glob("*.kml") + glob.glob("*.kmz")

if not lista_mapas:
    raise FileNotFoundError("ERRO: Nenhum mapa KML ou KMZ encontrado na raiz.")

arquivo_mapa = lista_mapas[0]
print(f"    Mapa Original: {arquivo_mapa}")

# A. Tratamento de KMZ (Extração para Cache)
if arquivo_mapa.lower().endswith('.kmz'):
    with zipfile.ZipFile(arquivo_mapa, 'r') as z:
        kml_files = [f for f in z.namelist() if f.lower().endswith('.kml')]
        if not kml_files: raise RuntimeError("KMZ inválido.")

        # Extrai para dentro da pasta CACHE
        z.extract(kml_files[0], path=pasta_cache)
        arquivo_leitura = os.path.join(pasta_cache, kml_files[0])
        print(f"    > KMZ extraído em cache: {arquivo_leitura}")
else:
    arquivo_leitura = arquivo_mapa

# B. Leitura Multi-Camada
camadas = fiona.listlayers(arquivo_leitura)
print(f"    > Camadas detectadas: {len(camadas)}")

lista_gdfs = []
for camada in camadas:
    try:
        # engine='fiona' resolve o erro de caracteres especiais
        gdf_temp = gpd.read_file(arquivo_leitura, driver='KML', layer=camada, engine='fiona')
        if not gdf_temp.empty:
            gdf_temp['CAMADA_ORIGEM'] = camada
            cols_uteis = [c for c in gdf_temp.columns if c in ['Name', 'geometry', 'CAMADA_ORIGEM']]
            lista_gdfs.append(gdf_temp[cols_uteis])
    except Exception as e:
        print(f"      [AVISO] Erro na camada '{camada}': {e}")

# C. Consolidação
if lista_gdfs:
    gdf_mapa_oficial = pd.concat(lista_gdfs, ignore_index=True)

    # Força SIRGAS 2000
    if gdf_mapa_oficial.crs != "EPSG:4674":
        print("    > Convertendo projeção para SIRGAS 2000...")
        gdf_mapa_oficial = gdf_mapa_oficial.to_crs("EPSG:4674")

    # Salva GeoPackage Oficial (Dentro do cache para não poluir)
    nome_gpkg = "MAPA_TERRITORIAL_OFICIAL.gpkg"
    caminho_gpkg = os.path.join(pasta_cache, nome_gpkg)

    gdf_mapa_oficial.to_file(caminho_gpkg, driver="GPKG")
    print(f"    > Mapa consolidado salvo em: {caminho_gpkg} ({len(gdf_mapa_oficial)} polígonos)")
else:
    raise RuntimeError("Nenhuma camada válida extraída.")

# --- 3. PROJETA OS POLÍGONOS E PONTOS PARA VALIDAÇÃO ---

# Cria exibição temporária (diferente da SIRGAS 2000)
mapa_visual = gdf_mapa_oficial.to_crs(epsg=4326)

# Centraliza o mapa
centro = mapa_visual.to_crs(epsg=3857).geometry.centroid.to_crs(epsg=4326).iloc[0]

# Cria o mapa centralizado
m = folium.Map(location=[centro.y, centro.x], zoom_start=13)

# Adiciona os polígonos
folium.GeoJson(
    mapa_visual,
    name='Territórios',
    style_function=lambda x: {'fillColor': '#3388ff', 'color': '#000000', 'weight': 1.5, 'fillOpacity': 0.1},
    tooltip=folium.GeoJsonTooltip(fields=['Name', 'CAMADA_ORIGEM'], aliases=['Nome:', 'Grupo:'])
).add_to(m)

if 'df_dados' in globals() and len(df_dados) > 0 and 'NU_ANO' in df_dados.columns:

    df_mapa = df_dados[df_dados['GEO_STATUS'] == 'END_VALIDO'].copy()

    print(f"    Processando {len(df_mapa)} pontos válidos para visualização...")

    if len(df_mapa) > 0:

        # Cria GeoDataFrame temporário usando df_mapa
        gdf_todos_pontos = gpd.GeoDataFrame(
            df_mapa,
            geometry=gpd.points_from_xy(df_mapa.X, df_mapa.Y),
            crs="EPSG:4674" # Origem SIRGAS
        ).to_crs(epsg=4326) # Destino GPS

        # Identifica anos disponíveis
        anos_disponiveis = sorted(gdf_todos_pontos['NU_ANO'].dropna().unique())
        ultimo_ano = anos_disponiveis[-1] if anos_disponiveis else None

        print(f"    > Anos identificados: {anos_disponiveis}")

        cores_ano = ['red', 'orange', 'green', 'purple', 'cadetblue', 'darkred']

        for i, ano in enumerate(anos_disponiveis):
            gdf_ano = gdf_todos_pontos[gdf_todos_pontos['NU_ANO'] == ano]
            qtd = len(gdf_ano)
            visivel = (ano == ultimo_ano)

            nome_camada = f"Casos {ano} (n={qtd})"
            grupo_ano = folium.FeatureGroup(name=nome_camada, show=visivel)
            cor_ponto = cores_ano[i % len(cores_ano)]

            for _, row in gdf_ano.iterrows():
                folium.CircleMarker(
                    location=[row.geometry.y, row.geometry.x],
                    radius=3,
                    color=cor_ponto,
                    fill=True,
                    fill_color=cor_ponto,
                    fill_opacity=0.7,
                    popup=f"Ano: {ano}<br>Notif: {row.get('NU_NOTIFIC')}"
                ).add_to(grupo_ano)

            grupo_ano.add_to(m)
            print(f"      [ADD] Camada '{nome_camada}' gerada.")

    else:
        # Este else pertence ao if len(df_mapa) > 0
        print("    [AVISO] Nenhum caso com 'END_VALIDO' encontrado para plotar.")

else:
    # Este else pertence ao if inicial ('df_dados' in globals...)
    print("    [AVISO] Não há dados em 'df_dados' ou coluna NU_ANO não encontrada.")

# Exibe o mapa para verificação
folium.LayerControl(collapsed=False).add_to(m)
m

# --- 4. JUNÇÃO ESPACIAL DOS CASOS E POLÍGONOS ---

# Configurações iniciais
if 'df_dados' not in globals() or 'gdf_mapa_oficial' not in globals():
    raise NameError("ERRO: As variáveis 'df_dados' ou 'gdf_mapa_oficial' não estão na memória.")

pasta_destino = "verificacao"
os.makedirs(pasta_destino, exist_ok=True)
data_hoje = datetime.now().strftime('%Y%m%d')

# Parâmetros de vinculação
DISTANCIA_LIMITE_METROS = 50  # Raio de repescagem
CRS_METRICO = "EPSG:5880"     # SIRGAS 2000 / Brazil Polyconic (para calcular metros)

# Separa apenas o que tem coordenadas para a análise espacial (Evita erro com NaN)
mask_geo_valido = (df_dados['X'].notnull()) & (df_dados['Y'].notnull())
df_geo_apto = df_dados[mask_geo_valido].copy()

# Prepara os Casos Aptos (SIRGAS 2000)
gdf_casos = gpd.GeoDataFrame(
    df_geo_apto,
    geometry=gpd.points_from_xy(df_geo_apto.X, df_geo_apto.Y),
    crs="EPSG:4674"
)

# Prepara o Mapa (SIRGAS 2000)
if gdf_mapa_oficial.crs != "EPSG:4674":
    print("    > Ajustando CRS do mapa para SIRGAS 2000...")
    gdf_mapa_oficial = gdf_mapa_oficial.to_crs("EPSG:4674")

# 4.1 JUNÇÃO POR INTERSEÇÃO (WITHIN)
print("    > Rodada 1: Interseção exata (Within)...")
gdf_within = gpd.sjoin(
    gdf_casos,
    gdf_mapa_oficial[['Name', 'geometry']],
    how='left',
    predicate='within'
)

# Separa sucesso de falha
mask_sucesso = gdf_within['Name'].notnull()
gdf_sucesso = gdf_within[mask_sucesso].copy()
gdf_sucesso['METODO_VINCULO'] = 'INTRA_POLIGONO'
gdf_sucesso = gdf_sucesso.rename(columns={'Name': 'TERRITORIO_VINCULADO'})

gdf_orfaos = gdf_within[~mask_sucesso].drop(columns=['Name', 'index_right'])
print(f"      - Vinculados: {len(gdf_sucesso)} | Não Vinculados: {len(gdf_orfaos)}")

# 4.2 REPESCAGEM (NEAREST NEIGHBOR)
if len(gdf_orfaos) > 0:
    print(f"    > Rodada 2: Repescagem (Nearest < {DISTANCIA_LIMITE_METROS}m)...")

    # Projeta para Metros temporariamente
    orfaos_metros = gdf_orfaos.to_crs(CRS_METRICO)
    mapa_metros = gdf_mapa_oficial.to_crs(CRS_METRICO)

    # Busca vizinho mais próximo
    gdf_nearest = gpd.sjoin_nearest(
        orfaos_metros,
        mapa_metros[['Name', 'geometry']],
        how='left',
        max_distance=DISTANCIA_LIMITE_METROS,
        distance_col='DISTANCIA_METROS'
    )

    # Filtra resgatados
    mask_resgatados = gdf_nearest['Name'].notnull()
    gdf_resgatados = gdf_nearest[mask_resgatados].copy()

    # Formata
    if not gdf_resgatados.empty:
        gdf_resgatados['METODO_VINCULO'] = 'RAIO_TOLERANCIA'
        gdf_resgatados = gdf_resgatados.rename(columns={'Name': 'TERRITORIO_VINCULADO'})
        # Volta para CRS original
        gdf_resgatados = gdf_resgatados.to_crs("EPSG:4674").drop(columns=['DISTANCIA_METROS', 'index_right'])
        print(f"      - Resgatados: {len(gdf_resgatados)}")
    else:
        print("      - Nenhum ponto resgatado na tolerância.")
        gdf_resgatados = gpd.GeoDataFrame()

    # Consolida os dados espaciais (Sucesso + Resgatados)
    gdf_resultados_espaciais = pd.concat([gdf_sucesso, gdf_resgatados], ignore_index=False)
else:
    gdf_resultados_espaciais = gdf_sucesso.copy()


# 4.3 CONSOLIDAÇÃO
# Prepara dataframe de referência (apenas as colunas novas)
df_ref_vinculo = pd.DataFrame(gdf_resultados_espaciais[['TERRITORIO_VINCULADO', 'METODO_VINCULO']])

# Merge à Esquerda no df_dados original:
# Mantém TODAS as linhas de df_dados, preenchendo vinculação onde houver
df_auditoria = df_dados.join(df_ref_vinculo, how='left')

# --- RELATÓRIO E ARQUIVO DE VERIFICACÃO ---
print("    > Gerando CSV Analítico...")
nome_csv = f"verificacao_etapa3_casos_{data_hoje}.csv"
caminho_csv = os.path.join(pasta_destino, nome_csv)
df_auditoria.to_csv(caminho_csv, sep=';', index=False, encoding='utf-8-sig')


# --- 5. CONTAGEM DE CASOS POR POLÍGONOS ---

# Filtra apenas quem tem vínculo para a contagem
df_vinculados = df_auditoria[df_auditoria['TERRITORIO_VINCULADO'].notnull()].copy()
df_vinculados['NU_ANO'] = df_vinculados['NU_ANO'].fillna('SEM_ANO').astype(str)

# Pivot Table
df_pivot = df_vinculados.pivot_table(
    index='TERRITORIO_VINCULADO',
    columns='NU_ANO',
    aggfunc='size',
    fill_value=0
)
df_pivot['TOTAL_GERAL'] = df_pivot.sum(axis=1)

# Merge com Mapa
gdf_agregado = gdf_mapa_oficial.merge(
    df_pivot,
    left_on='Name',
    right_index=True,
    how='left'
)
cols_anos = list(df_pivot.columns)
gdf_agregado[cols_anos] = gdf_agregado[cols_anos].fillna(0).astype(int)

# Salva GeoPackage
nome_gpkg = f"verificacao_etapa3_poligonos_{data_hoje}.gpkg"
caminho_gpkg = os.path.join(pasta_destino, nome_gpkg)
gdf_agregado.to_file(caminho_gpkg, driver="GPKG")


# --- RELATÓRIO E ARQUIVO DE VERIFICACÃO ---
print("\n" + "="*50)
print("RELATÓRIO FINAL DE GEOPROCESSAMENTO")
print("="*50)
df_auditoria['CLASSI_FIN'] = df_auditoria['CLASSI_FIN'].astype(str)

# Totais Absolutos
total_casos_geral = len(df_auditoria)

# Filtro de Casos Prováveis (Universo da Doença)
provaveis_df = df_auditoria[df_auditoria['CLASSI_FIN'] != '5']
total_provaveis = len(provaveis_df)

# Filtro de Endereços Válidos (Universo do Mapa)
provaveis_validos = provaveis_df[provaveis_df['GEO_STATUS'] == 'END_VALIDO']
total_validos = len(provaveis_validos)

# Filtro de Vinculados (Sucesso)
provaveis_vinculados = provaveis_df[provaveis_df['TERRITORIO_VINCULADO'].notnull()]
total_vinculados = len(provaveis_vinculados)

# Detalhe dos Métodos
qtd_within = len(provaveis_vinculados[provaveis_vinculados['METODO_VINCULO'] == 'INTRA_POLIGONO'])
qtd_nearest = len(provaveis_vinculados[provaveis_vinculados['METODO_VINCULO'] == 'RAIO_TOLERANCIA'])

# Função auxiliar para evitar divisão por zero
def calc_pct(valor, total):
    return (valor / total * 100) if total > 0 else 0

# A. Para o Bloco "Resultado da Atribuição" (Base = END_VALIDO)
qtd_perda_geo = total_validos - total_vinculados

pct_within_geo = calc_pct(qtd_within, total_validos)
pct_nearest_geo = calc_pct(qtd_nearest, total_validos)
pct_perda_geo = calc_pct(qtd_perda_geo, total_validos)

# B. Para o Bloco "Totais Gerais" (Base = Total de Prováveis)
provaveis_com_coords = provaveis_df[provaveis_df['X'].notnull()]
total_coords = len(provaveis_com_coords)

pct_coords = calc_pct(total_coords, total_provaveis)
pct_validos_total = calc_pct(total_validos, total_provaveis)
pct_vinculados_final = calc_pct(total_vinculados, total_provaveis) # Indicador Final

print("RESULTADO DA ATRIBUIÇÃO (Base: Casos com END_VALIDO):")
print(f"INTERSECÇÃO           : {qtd_within} ({pct_within_geo:.1f}%)")
print(f"PROXIMIDADE {DISTANCIA_LIMITE_METROS} METROS : {qtd_nearest} ({pct_nearest_geo:.1f}%)")
print(f"PERDA (Fora do Mapa)  : {qtd_perda_geo} ({pct_perda_geo:.1f}%)")
print("-" * 50)

print(f"TOTAL DE CASOS (Bruto)                          : {total_casos_geral}")
print(f"TOTAL DE PROVÁVEIS (100%)                       : {total_provaveis}")
print(f"TOTAL DE PROVÁVEIS COM COORDENADAS ATRIBUIDAS   : {total_coords} ({pct_coords:.1f}%)")
print(f"TOTAL DE END_VALIDO                             : {total_validos} ({pct_validos_total:.1f}%)")
print(f"TOTAL COM VINCULAÇÃO A UM POLÍGONO              : {total_vinculados} ({pct_vinculados_final:.1f}%)")
print("-" * 50)

print(f"APROVEITAMENTO % FINAL DE CASOS                 : {pct_vinculados_final:.2f}%")
print("-" * 50)

total_poligonos_mapa = len(gdf_mapa_oficial)
total_poligonos_com_casos = len(df_pivot)
print(f"TOTAL DE POLÍGONOS                              : {total_poligonos_mapa}")
print(f"TOTAL DE POLÍGONOS COM CASOS                    : {total_poligonos_com_casos}")
print("="*50)

print(f"\nArquivos Gerados:")
print(f"CSV Auditoria: {os.path.basename(caminho_csv)}")
print(f"Mapa GPKG:     {os.path.basename(caminho_gpkg)}")

"""# Análise de Gi*"""

# IMPORTAÇÃO DE BIBLIOTECAS

# 1. Sistema Operacional e Arquivos
import os
import shutil
import glob
import warnings
from datetime import datetime
from google.colab import files

# 2. Processamento de Dados
import pandas as pd
import numpy as np

# 3. Inteligência Espacial e Mapas
import geopandas as gpd
import folium

# 4. Estatística Espacial (PySAL)
# Importamos apenas as funções que realmente usamos nas rotinas
import libpysal
from libpysal.weights import Queen, DistanceBand, KNN, fill_diagonal
from esda.getisord import G_Local

# 5. Estatística Matemática e Machine Learning
from scipy.stats import zscore               # Para padronização (Rotina 4A)
from sklearn.neighbors import NearestNeighbors    # Para Raio Dinâmico (Rotina 4B)
from statsmodels.stats.multitest import multipletests # Para correção FDR (Rotina 4B)

# 6. Plotagem de mapas e visualização de imagens
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
from matplotlib.colors import ListedColormap
import seaborn as sns
from IPython.display import Image, display

# CONFIGURAÇÕES INICIAIS

print(">>> Iniciando... Carregamento e Correção de Geometria...")

# SETUP DE DIRETÓRIOS
pasta_verificacao = "verificacao"
pasta_cache = "cache"
os.makedirs(pasta_verificacao, exist_ok=True)
os.makedirs(pasta_cache, exist_ok=True)

# LOCALIZAÇÃO DO ARQUIVO BRUTO
gdf_raw = None

if 'gdf_agregado' in globals() and isinstance(gdf_agregado, gpd.GeoDataFrame):
    print("    [FONTE] Usando 'gdf_agregado' da memória.")
    gdf_raw = gdf_agregado.copy()

else:
    # Prioridade para arquivos com nome padrão, depois qualquer gpkg
    busca = sorted(glob.glob("*.gpkg"))
    if busca:
        arquivo_alvo = busca[0]
        print(f"    [FONTE] Carregando arquivo do disco: {arquivo_alvo}")
        gdf_raw = gpd.read_file(arquivo_alvo)
    else:
        raise FileNotFoundError("ERRO: Nenhum arquivo .gpkg encontrado. Faça o upload do arquivo bruto.")

# --- CORREÇÃO TOPOLÓGICA (O BLOCO QUE FALTAVA) ---
print("\n>>> Aplicando Protocolo de Correção Geométrica...")

# Cria a variável de análise a partir do bruto
gdf_analise = gdf_raw.copy()

# A. Corrige auto-intersecções (nós de laço)
gdf_analise.geometry = gdf_analise.geometry.make_valid()

# B. Fecha micro-fissuras (gaps) entre polígonos
gdf_analise.geometry = gdf_analise.geometry.buffer(0)

# Verificação de CRS (apenas alerta)
if gdf_analise.crs is None:
    print("    [AVISO] O arquivo não possui Sistema de Coordenadas (CRS).")

# --- RELATÓRIO E ARQUIVO DE VERIFICAÇÃO ---
# Agora gdf_analise existe e pode ser lido
colunas_anos = [str(c) for c in gdf_analise.columns if str(c).isdigit() and len(str(c)) == 4]
colunas_anos.sort()

if not colunas_anos:
    raise ValueError("ERRO: Colunas de anos (ex: 2021, 2022) não encontradas.")

# Salva o arquivo na pasta cache
caminho_final = os.path.join(pasta_cache, "dataset_pronto_analise.gpkg")
if os.path.exists(caminho_final):
    os.remove(caminho_final)
gdf_analise.to_file(caminho_final, driver="GPKG")

print("-" * 50)
print("ETAPA 1 CONCLUÍDA COM SUCESSO")
print("-" * 50)
print(f"Arquivo Original : {arquivo_alvo if 'arquivo_alvo' in locals() else 'Memória'}")
print(f"Polígonos        : {len(gdf_analise)}")
print(f"Série Histórica  : {colunas_anos}")
print(f"Correção         : Applied make_valid() & buffer(0)")
print(f"Arquivo Gerado   : {caminho_final}")
print("-" * 50)

# --- 2. ANÁLISE DA DISTRIBUIÇÃO DO NÚMERO DE CASOS POR POLÍGONO ---

print(">>> Iniciando Diagnóstico de Distribuição de Casos...")

pasta_verificacao = "verificacao"
os.makedirs(pasta_verificacao, exist_ok=True)

if 'gdf_analise' not in globals():
    raise SystemError("ERRO: Base de dados não encontrada.")

df_stats = gdf_analise.copy()

# Identifica colunas de anos
cols_anos = [c for c in df_stats.columns if c.isdigit() and len(c) == 4]
df_stats['TOTAL_GERAL'] = df_stats[cols_anos].sum(axis=1)

# Ordenação e Percentuais
df_sorted = df_stats.sort_values(by='TOTAL_GERAL', ascending=False)
total_abs = df_sorted['TOTAL_GERAL'].sum()
df_sorted['PCT_ACUM'] = (df_sorted['TOTAL_GERAL'].cumsum() / total_abs) * 100

# Estatísticas Descritivas (Percentis)
stats = df_sorted['TOTAL_GERAL'].describe(percentiles=[.1, .25, .5, .75, .9])

print(f"\n    [PARÂMETROS DA DISTRIBUIÇÃO - N={len(df_sorted)}]")
print(f"    - P10 (Silenciosos)  : {stats['10%']:.1f} casos")
print(f"    - P50 (Mediana)      : {stats['50%']:.1f} casos")
print(f"    - P90 (Concentradores): {stats['90%']:.1f} casos")
print(f"    - Média Geral        : {stats['mean']:.1f} casos")

# VIZUALIZAÇÃO

# Histograma
caminho_hist = os.path.join(pasta_verificacao, "diag_A_distribuicao_casos.png")
plt.figure(figsize=(10, 5))
sns.histplot(df_sorted['TOTAL_GERAL'], bins=20, kde=True, color='steelblue')
plt.axvline(stats['50%'], color='k', linestyle='--', label=f"Mediana ({stats['50%']:.0f})")
plt.axvline(stats['90%'], color='r', linestyle='--', label=f"P90 ({stats['90%']:.0f})")
plt.title('Distribuição de Casos Acumulados')
plt.legend()
plt.tight_layout()
plt.savefig(caminho_hist)
plt.close()

# Boxplot dos intervalos anuais
# Reduzi para Top/Bottom 15 para melhor visualização em N=33
df_melt = df_sorted.melt(id_vars=['Name'], value_vars=cols_anos, var_name='Ano', value_name='Casos')

def plot_box(df_data, titulo, arquivo, palette):
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='Name', y='Casos', data=df_data, hue='Name', palette=palette, legend=False)
    plt.xticks(rotation=45, ha='right')
    plt.title(titulo)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(arquivo)
    plt.close()

# Top 15 (Focos Principais)
top_names = df_sorted.head(15)['Name']
plot_box(df_melt[df_melt['Name'].isin(top_names)],
         "Variação Anual - TOP 15 (Maiores Focos)",
         os.path.join(pasta_verificacao, "diag_B_top_variacao.png"), "Reds_r")

# Bottom 15 (Menores Focos)
bot_names = df_sorted.tail(15)['Name']
plot_box(df_melt[df_melt['Name'].isin(bot_names)],
         "Variação Anual - BOTTOM 15 (Menores Focos)",
         os.path.join(pasta_verificacao, "diag_C_bottom_variacao.png"), "Blues_r")

print("\n    > Gráficos gerados (Histograma e Boxplots de Variação).")
try:
    display(Image(filename=caminho_hist))
except:
    pass

# Tabela de percentis
print("-" * 70)
print(f"{'RANK':<5} | {'LOCALIDADE':<35} | {'TOTAL':<8} | {'% ACUM':<8}")
print("-" * 70)

# Loop único para identificar extremos
for i, (idx, row) in enumerate(df_sorted.iterrows(), 1):
    nome = str(row['Name'])[:35]
    total = row['TOTAL_GERAL']
    pct = row['PCT_ACUM']

    # Marcação visual
    if total >= stats['90%']:
        tag = " [P90 - ALTO]"
    elif total <= stats['10%']:
        tag = " [P10 - BAIXO]"
    else:
        continue # Pula o "miolo" para não poluir o log

    print(f"{i:<5} | {nome:<35} | {total:<8.0f} | {pct:<6.1f}% {tag}")

print("-" * 70)
print("NOTA: O P90 indica os locais que concentram a epidemia.")
print("      O P10 indica locais com silêncio epidemiológico ou subnotificação.")
print("-" * 70)

# --- 3. AVALIAÇÃO DA TOPOLOGIA E VIZINHANÇA ---

print(">>> Iniciando Auditoria Completa: Conectividade, Distâncias e Dimensão...")

pasta_verificacao = "verificacao"
os.makedirs(pasta_verificacao, exist_ok=True)

# Garante que a variável esteja na memória
if 'gdf_analise' not in globals():
    raise SystemError("ERRO: Variável 'gdf_analise' não encontrada. Execute a Etapa 1.")

# VERIFICAÇÃO DE INTEGRIDADE DA BASE
print("    > Validando estrutura do arquivo para análise Gi*...")
erros_estrutura = []

# Identificação inteligente da coluna de ID
candidatos_nome = ['Name', 'NOME', 'NM_BAIRRO', 'name', 'nome']
col_id_alvo = next((c for c in candidatos_nome if c in gdf_analise.columns), gdf_analise.columns[0])

cols_anos = [c for c in gdf_analise.columns if str(c).isdigit() and len(str(c)) == 4]
if not cols_anos:
    erros_estrutura.append("Nenhuma coluna de Ano (ex: '2021') encontrada.")

if erros_estrutura:
    print(f"    [!] ALERTA DE ESTRUTURA: {', '.join(erros_estrutura)}")
else:
    print(f"    [OK] Estrutura válida. ID: '{col_id_alvo}' | Anos: {cols_anos}")

# ANÁLISE DE CONECTIVIDADE (VIZINHOS - QUEEN)
print("\n    > Calculando relações de vizinhança (Queen - Contiguidade)...")
w_check = Queen.from_dataframe(gdf_analise, silence_warnings=True, use_index=True)

cardinalidades = pd.Series(w_check.cardinalities, index=gdf_analise.index)
total_localidades = len(gdf_analise)
com_4_mais = (cardinalidades >= 4).sum()
pct_4_mais = (com_4_mais / total_localidades) * 100
ilhas = w_check.islands

print(f"\n    [A. RELATÓRIO DE CONECTIVIDADE FÍSICA]")
print(f"    - Total de Localidades      : {total_localidades}")
print(f"    - Localidades com >= 4 Viz. : {com_4_mais} ({pct_4_mais:.1f}%)")
print(f"    - Ilhas (Sem vizinho físico): {len(ilhas)}")

# ANÁLISE DE DISTÂNCIAS (KNN - RAIO IDEAL)
print("\n    > Calculando distâncias euclidianas entre centróides (KNN=1)...")

# [CORREÇÃO] Sanitização: Remove geometrias vazias antes do KNN
gdf_utm = gdf_analise.to_crs(gdf_analise.estimate_utm_crs())
gdf_utm = gdf_utm[~gdf_utm.is_empty & gdf_utm.geometry.notna()].copy() # <--- O PULO DO GATO

if len(gdf_utm) < len(gdf_analise):
    print(f"      [AVISO] {len(gdf_analise) - len(gdf_utm)} geometrias vazias ignoradas no cálculo de distância.")

# Cálculo do vizinho mais próximo
knn = KNN.from_dataframe(gdf_utm, k=1)
distancias = []
nomes_vizinhos = []

for i in range(len(gdf_utm)):
    # Geometria A
    ponto_a = gdf_utm.geometry.iloc[i].centroid
    # Vizinho mais próximo (Geometria B)
    # knn.neighbors retorna um dicionário {index: [vizinho1, vizinho2]}
    # Usamos o iloc para mapear o índice posicional
    id_origem = gdf_utm.index[i]
    id_viz = knn.neighbors[id_origem][0]

    # Busca a geometria do vizinho pelo índice
    ponto_b = gdf_utm.loc[id_viz].geometry.centroid

    # Distância e Nome
    d = ponto_a.distance(ponto_b)
    distancias.append(d)
    nomes_vizinhos.append(gdf_utm.loc[id_viz, col_id_alvo])

# Adiciona ao dataframe temporário para análise
df_dist = pd.DataFrame({
    'Localidade': gdf_utm[col_id_alvo].values,
    'Vizinho_Mais_Prox': nomes_vizinhos,
    'Distancia_Metros': distancias
})

# Estatísticas
dist_min = df_dist['Distancia_Metros'].min()
dist_med = df_dist['Distancia_Metros'].median()
dist_max = df_dist['Distancia_Metros'].max() # O CRÍTICO
dist_mean = df_dist['Distancia_Metros'].mean()

print(f"\n    [B. RELATÓRIO DE DISTÂNCIA CRÍTICA (RAIO IDEAL)]")
print(f"    - Distância Média (Urbano)  : {dist_mean:.2f} m")
print(f"    - Distância Mediana         : {dist_med:.2f} m")
print(f"    - Distância Máxima (CRÍTICA): {dist_max:.2f} m  <--- PISO PARA O MODELO 4C/4D")

print("\n    > TOP 5 - BAIRROS MAIS ISOLADOS (Definem o Raio):")
isolados = df_dist.sort_values(by='Distancia_Metros', ascending=False).head(5)
print(f"    {'LOCALIDADE':<30} | {'VIZINHO':<20} | {'DISTÂNCIA':<10}")
print("    " + "-"*65)
for _, row in isolados.iterrows():
    print(f"    {str(row['Localidade'])[:30]:<30} | {str(row['Vizinho_Mais_Prox'])[:20]:<20} | {row['Distancia_Metros']:.2f} m")

# ANÁLISE DE HOMOGENEIDADE TERRITORIAL (ÁREA)
print("\n    > Calculando áreas (Hectares)...")
areas_ha = gdf_utm.area / 10000

area_min = areas_ha.min()
area_max = areas_ha.max()
area_med = areas_ha.median()
cv_area = (areas_ha.std() / areas_ha.mean()) * 100

print(f"\n    [C. RELATÓRIO DE DIMENSÃO TERRITORIAL]")
print(f"    - Área Mínima   : {area_min:.2f} ha")
print(f"    - Área Mediana  : {area_med:.2f} ha")
print(f"    - Área Máxima   : {area_max:.2f} ha")
print(f"    - Disparidade (CV): {cv_area:.1f}%")

if cv_area > 100:
    print("    [!] ALERTA: Polígonos muito díspares (risco de viés MAUP).")
else:
    print("    [OK] Homogeneidade territorial aceitável.")

# VISUALIZAÇÕES

# GRÁFICO 1: Histograma de Vizinhos (Queen)
caminho_hist_viz = os.path.join(pasta_verificacao, "diag_A_histograma_vizinhos.png")
plt.figure(figsize=(10, 5))
max_viz = int(cardinalidades.max()) if not cardinalidades.empty else 0
sns.histplot(cardinalidades, bins=range(0, max_viz + 2), color='steelblue', discrete=True)
plt.axvline(4, color='orange', linestyle='--', label='Meta (4 Vizinhos)')
plt.title(f'Distribuição de Conectividade Física (Queen)')
plt.xlabel('Número de Vizinhos Contíguos')
plt.ylabel('Qtd Localidades')
plt.legend()
plt.tight_layout()
plt.savefig(caminho_hist_viz)
plt.close()

# GRÁFICO 2: Histograma de Distâncias (NOVO)
caminho_hist_dist = os.path.join(pasta_verificacao, "diag_B_histograma_distancias.png")
plt.figure(figsize=(10, 5))
sns.histplot(df_dist['Distancia_Metros'], kde=True, color='purple')
plt.axvline(dist_max, color='red', linestyle='--', label=f'Crítica ({dist_max:.0f}m)')
plt.title(f'Distribuição de Distâncias entre Vizinhos (KNN=1)')
plt.xlabel('Distância (Metros)')
plt.ylabel('Frequência')
plt.legend()
plt.tight_layout()
plt.savefig(caminho_hist_dist)
plt.close()

# GRÁFICO 3: Mapa de Vulnerabilidade
caminho_mapa = os.path.join(pasta_verificacao, "diag_C_mapa_critico.png")

# Separação dos grupos
islands_gdf = gdf_analise[cardinalidades == 0]
low_connect_gdf = gdf_analise[(cardinalidades > 0) & (cardinalidades < 4)]

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
gdf_analise.plot(ax=ax, color='whitesmoke', edgecolor='#cccccc')

if not low_connect_gdf.empty:
    low_connect_gdf.plot(ax=ax, color='#ffcccc', edgecolor='red', hatch='///')
if not islands_gdf.empty:
    islands_gdf.plot(ax=ax, color='#9370DB', edgecolor='indigo', hatch='XX')

legend_elements = [
    Line2D([0], [0], marker='s', color='w', markerfacecolor='whitesmoke', markeredgecolor='#cccccc', label='Conectividade OK'),
    Line2D([0], [0], marker='s', color='w', markerfacecolor='#ffcccc', markeredgecolor='red', label=f'Baixa Conectividade (<4): {len(low_connect_gdf)}'),
    Line2D([0], [0], marker='s', color='w', markerfacecolor='#9370DB', markeredgecolor='indigo', label=f'Ilhas (0 viz): {len(islands_gdf)}')
]
ax.legend(handles=legend_elements, loc='lower right')
ax.set_title("Diagnóstico: Vulnerabilidade Topológica")
ax.axis('off')
plt.savefig(caminho_mapa, dpi=150, bbox_inches='tight')
plt.close()

print(f"\n    > Gerando gráficos de diagnóstico...")
try:
    display(Image(filename=caminho_hist_viz))
    display(Image(filename=caminho_hist_dist))
    display(Image(filename=caminho_mapa))
except:
    pass

print("-" * 60)
print(f"AUDITORIA CONCLUÍDA.")
print("-" * 60)

# --- 4A. PADRÃO MINISTÉRIO DA SAÚDE: QUEEN 1ª ORDEM (P<0.05 e P<0.10) ---

print(">>> [ANÁLISE QUEEN 1ª ORDEM] Iniciando processamento unificado...")
print(">>> Parâmetros: Z-Score Padronizado (MS) | Cortes p<0.05 e p<0.10")

# SETUP DE DIRETÓRIOS
vars_to_del = ['w_queen', 'gdf_analise', 'df_compilado']
for var in vars_to_del:
    if var in globals(): del globals()[var]

pasta_saida = "resultados_finais_4A"
pasta_cache = "cache"

if os.path.exists(pasta_saida): shutil.rmtree(pasta_saida)
os.makedirs(pasta_saida)
os.makedirs(pasta_cache, exist_ok=True)

# CARREGAMENTO DOS DADOS
caminho_dados = os.path.join(pasta_cache, "dataset_pronto_analise.gpkg")
if not os.path.exists(caminho_dados): raise SystemError("Base não encontrada no cache.")
gdf_analise = gpd.read_file(caminho_dados)

# Identificadores
candidatos_nome = ['Name', 'NOME', 'NM_BAIRRO', 'name', 'nome']
col_id = next((c for c in candidatos_nome if c in gdf_analise.columns), gdf_analise.columns[0])
gdf_analise[col_id] = gdf_analise[col_id].astype(str).str.strip()

colunas_anos = [str(c) for c in gdf_analise.columns if str(c).isdigit() and len(str(c)) == 4]
colunas_anos.sort()

# MATRIZ DE PESOS (QUEEN 1ª ORDEM + AUTO-INCLUSÃO)
print("\n[1/3] Calculando Matriz Queen...")
w_queen = Queen.from_dataframe(gdf_analise, silence_warnings=True, use_index=True)
w_queen = fill_diagonal(w_queen, val=1) # Obrigatório para Gi*
w_queen.transform = 'R'

# Salva no cache
caminho_gal = os.path.join(pasta_cache, "matriz_pesos_queen.gal")
try:
    gal_file = libpysal.io.open(caminho_gal, 'w')
    gal_file.write(w_queen)
    gal_file.close()
    print(f"      > Matriz salva em: {caminho_gal}")
except: pass

# LOOP DE CÁLCULO (ESTRUTURA DE DADOS UNIFICADA)
print("\n[2/3] Executando estatística (Z Padronizado)...")

# Dataframe Mestre
df_master = gdf_analise[[col_id, 'geometry']].copy()

# Inicializa contadores
df_master['TOTAL_0.05'] = 0
df_master['TOTAL_0.1'] = 0

np.seterr(all='ignore')

for ano in colunas_anos:
    try:
        valores_casos = gdf_analise[ano].fillna(0).astype(float).values
        if valores_casos.sum() == 0: continue
    except: continue

    # A. Cálculos Estatísticos
    # 1. Escore Z (Padrão Estatístico)
    z_bruto = zscore(valores_casos)
    z_bruto = np.nan_to_num(z_bruto, nan=0.0)

    # 2. Escore Z Padronizado (Exigência MS: Dividir pelo Máximo)
    z_max = z_bruto.max()
    z_std = z_bruto / z_max if z_max > 0 else z_bruto

    # 3. Gi* (Input: Z Padronizado)
    # star=True aqui com fill_diagonal acima é redundante, mas mantive seu script.
    gi = G_Local(z_std, w_queen, star=True)
    z_gi = np.nan_to_num(gi.Zs, nan=0.0)
    p_sim = np.nan_to_num(gi.p_sim, nan=1.0)

    # B. Identificação de Hotspots (Binário)
    # Regra: Z_Gi > 0 (Hotspot) E P-Valor < Corte
    hotspot_05 = ((z_gi > 0) & (p_sim < 0.05)).astype(int)
    hotspot_10 = ((z_gi > 0) & (p_sim < 0.10)).astype(int)

    # C. Armazenamento no DataFrame Mestre
    df_master[f'CASOS_{ano}'] = valores_casos
    df_master[f'Z_ESCORE_{ano}'] = np.round(z_bruto, 4)
    df_master[f'Z_PAD_{ano}'] = np.round(z_std, 4)
    df_master[f'VALOR_P_{ano}'] = np.round(p_sim, 5)

    # Salvando o status binário anual (p<0.05) para o mapa
    df_master[f'HOTSPOT_STATUS_{ano}'] = hotspot_05

    # D. Atualização dos Totais
    df_master['TOTAL_0.05'] += hotspot_05
    df_master['TOTAL_0.1'] += hotspot_10

# EXPORTAÇÃO DOS ARQUIVOS FINAIS
print("\n[3/3] Exportando resultados consolidados...")

# Define caminhos
path_gpkg = os.path.join(pasta_saida, "resultado_estratificacao_4A.gpkg")
path_xlsx = os.path.join(pasta_saida, "relatorio_analitico_4A.xlsx")

# Salva GPKG
df_master.to_file(path_gpkg, layer="COMPILADO_GERAL", driver="GPKG")

# Salva XLSX
df_excel = pd.DataFrame(df_master.drop(columns='geometry'))
df_excel.to_excel(path_xlsx, sheet_name="COMPILADO_GERAL", index=False)

# GERAÇÃO DO MAPA HTML (ESTILIZAÇÃO CATEGÓRICA SOLICITADA)
print("      > Gerando Mapa Interativo...")

# Projeção Web
df_mapa = df_master.to_crs(epsg=4326)

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    # Usa a projeção métrica original para achar o centroide geométrico correto
    centro_temp = gdf_analise.to_crs(gdf_analise.estimate_utm_crs()).geometry.centroid.to_crs(epsg=4326)
    y_cent = centro_temp.y.mean()
    x_cent = centro_temp.x.mean()

m = folium.Map(location=[y_cent, x_cent], zoom_start=13, tiles='CartoDB positron')

# --- DEFINIÇÃO DE ESTILOS ---

# Estilo 1: Recorrência (Cores Categóricas)
def get_style_recorrencia(feature, col_alvo):
    val = int(feature['properties'][col_alvo])
    estilo = {'weight': 1, 'color': 'black', 'opacity': 0.5, 'fillOpacity': 0.7}

    if val == 0:
        estilo['fillOpacity'] = 0 # Transparente
        return estilo

    # Regras Categóricas
    if val == 1: estilo['fillColor'] = '#FA8072' # 1 ano
    elif val <= 3: estilo['fillColor'] = '#FF0000' # 2 ou 3 anos
    elif val <= 5: estilo['fillColor'] = '#B22222' # 4 ou 5 anos
    else: estilo['fillColor'] = '#8B0000' # 6 anos ou mais

    return estilo

# [CORREÇÃO] Estilo 2: Camadas Anuais (Apenas p<0.05)
def get_style_anual(feature, col_alvo):
    val = int(feature['properties'][col_alvo])
    # Se 1 (Hotspot), Vermelho. Se 0, Transparente.
    if val == 1:
        return {'fillColor': '#FF0000', 'fillOpacity': 0.6, 'weight': 1, 'color': 'black', 'opacity': 0.5}
    else:
        return {'fillOpacity': 0, 'weight': 0.5, 'color': 'grey', 'opacity': 0.3}

total_anos_count = len(colunas_anos)

# --- ADIÇÃO DAS CAMADAS ---

# 1. Recorrência p < 0.05 (Padrão)
folium.GeoJson(
    df_mapa,
    name='Recorrência (p < 0.05)',
    style_function=lambda feature: get_style_recorrencia(feature, 'TOTAL_0.05'),
    tooltip=folium.GeoJsonTooltip(fields=[col_id, 'TOTAL_0.05'], aliases=['Local:', 'Anos (p<0.05):'])
).add_to(m)

# 2. Recorrência p < 0.10 (Desligada)
folium.GeoJson(
    df_mapa,
    name='Recorrência (p < 0.10)',
    style_function=lambda feature: get_style_recorrencia(feature, 'TOTAL_0.1'),
    tooltip=folium.GeoJsonTooltip(fields=[col_id, 'TOTAL_0.1'], aliases=['Local:', 'Anos (p<0.10):']),
    show=False
).add_to(m)

# 3. Camadas Anuais Individuais (Apenas p<0.05)
for ano in colunas_anos:
    col_status = f'HOTSPOT_STATUS_{ano}'
    folium.GeoJson(
        df_mapa,
        name=f'Hotspot {ano} (p<0.05)',
        style_function=lambda feature, c=col_status: get_style_anual(feature, c),
        tooltip=folium.GeoJsonTooltip(fields=[col_id, col_status], aliases=['Local:', f'Hotspot {ano}?']),
        show=False # Começam desligadas para não poluir
    ).add_to(m)

folium.LayerControl().add_to(m)

# Legenda HTML Ajustada
legend_html = f'''
<div style="position: fixed; bottom: 30px; left: 30px; z-index:9999; font-size:12px;
            background-color:white; padding: 10px; border: 1px solid grey; border-radius: 5px;">
  <b>Legenda de Recorrência</b><br>
  <i style="border:1px solid black; width:15px; height:15px; display:inline-block;"></i> 0 Anos (Transp.)<br>
  <i style="background:#FA8072; width:15px; height:15px; display:inline-block;"></i> 1 Ano<br>
  <i style="background:#FF0000; width:15px; height:15px; display:inline-block;"></i> 2 ou 3 Anos<br>
  <i style="background:#B22222; width:15px; height:15px; display:inline-block;"></i> 4 ou 5 Anos<br>
  <i style="background:#8B0000; width:15px; height:15px; display:inline-block;"></i> 6 Anos ou mais<br>
  <br>Total analisado: {total_anos_count} anos<br>
  <hr style="margin:5px 0;">
  <b>Camadas Anuais (p&lt;0.05)</b><br>
  <span style="color:#FF0000;">■</span> Hotspot Significativo
</div>
'''
m.get_root().html.add_child(folium.Element(legend_html))

m.save(os.path.join(pasta_saida, "MAPA_INTERATIVO_4A.html"))

print("\n" + "="*60)
print("PROCESSO CONCLUÍDO COM SUCESSO")
print("="*60)
print(f"Arquivos gerados em: '{pasta_saida}'")
print("1. XLSX: relatorio_analitico_4A.xlsx")
print("2. GPKG: resultado_estratificacao_4A.gpkg")
print("3. HTML: MAPA_INTERATIVO_4A.html (Com camadas anuais)")
print("-" * 60)
display(m)

# --- 4B. ANÁLISE PADRÃO ARCGIS: RAIO DINÂMICO + FDR + GI_BIN

# SETUP DE DIRETÓRIOS
vars_to_del = ['w_final', 'gdf_analise', 'df_master']
for var in vars_to_del:
    if var in globals(): del globals()[var]

pasta_saida = "resultados_finais_4B"
pasta_cache = "cache"

if os.path.exists(pasta_saida): shutil.rmtree(pasta_saida)
os.makedirs(pasta_saida)
os.makedirs(pasta_cache, exist_ok=True)

# 2. CARREGAMENTO DOS DADOS
caminho_dados = os.path.join(pasta_cache, "dataset_pronto_analise.gpkg")
if not os.path.exists(caminho_dados): raise SystemError("Base não encontrada no cache.")
gdf_analise = gpd.read_file(caminho_dados)

# Identificadores
candidatos_nome = ['Name', 'NOME', 'NM_BAIRRO', 'name', 'nome']
col_id = next((c for c in candidatos_nome if c in gdf_analise.columns), gdf_analise.columns[0])
gdf_analise[col_id] = gdf_analise[col_id].astype(str).str.strip()

colunas_anos = [str(c) for c in gdf_analise.columns if str(c).isdigit() and len(str(c)) == 4]
colunas_anos.sort()

# SANITIZAÇÃO DE GEOMETRIAS SEM CENTRÓIDE
print("\n[0/4] Verificando integridade das geometrias...")
qtd_inicial = len(gdf_analise)

# Remove geometrias nulas (None) e vazias (Empty Polygon)
gdf_analise = gdf_analise[gdf_analise.geometry.notna()]
gdf_analise = gdf_analise[~gdf_analise.geometry.is_empty]
gdf_analise = gdf_analise.reset_index(drop=True) # Resetar índice é vital para matriz de pesos

qtd_final = len(gdf_analise)
diff = qtd_inicial - qtd_final
if diff > 0:
    print(f"      [AVISO] {diff} linhas com geometria inválida/vazia foram removidas.")
else:
    print(f"      [OK] Todas as {qtd_final} geometrias são válidas.")

# 3. ENGENHARIA DE PESOS (RAIO DINÂMICO SKLEARN)
print("\n[1/4] Definindo Raio de Vizinhança...")

# A. Cálculo do Raio
# Agora garantimos que não há NaNs, então o .centroid.x vai funcionar
gdf_metros = gdf_analise.to_crs(gdf_analise.estimate_utm_crs())
coords = np.array(list(zip(gdf_metros.geometry.centroid.x, gdf_metros.geometry.centroid.y)))

nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(coords)
distancias, indices = nbrs.kneighbors(coords)
raio_matematico = distancias[:, 1].max()
threshold_final = max(raio_matematico * 1.01, 500.0)

print(f"      > Raio Calculado: {threshold_final:.2f} m")

# B. Geração da Matriz
w_final = DistanceBand.from_dataframe(gdf_metros, threshold=threshold_final, binary=True, silence_warnings=True)

# C. Salvar Matriz no Cache (.gwt)
caminho_matriz = os.path.join(pasta_cache, "matriz_pesos_raio_ajustado.gwt")
try:
    gal_file = libpysal.io.open(caminho_matriz, 'w')
    gal_file.write(w_final)
    gal_file.close()
except: pass

# D. Ajuste Final (Auto-Peso Obrigatório para Gi*)
w_final = fill_diagonal(w_final, val=1)
w_final.transform = 'R'

# 4. LOOP DE ANÁLISE ESTATÍSTICA
print("\n[2/4] Executando Gi* + FDR (Input: Casos Absolutos)...")

df_master = gdf_analise[[col_id, 'geometry']].copy()

def classificar_gi_bin(z, p_corrected):
    if p_corrected > 0.10: return 0
    if z > 0:
        if p_corrected <= 0.01: return 3
        if p_corrected <= 0.05: return 2
        return 1
    else:
        if p_corrected <= 0.01: return -3
        if p_corrected <= 0.05: return -2
        return -1

np.seterr(all='ignore')

for ano in colunas_anos:
    try:
        valores = gdf_analise[ano].fillna(0).astype(float).values
        if valores.sum() == 0: continue
    except: continue

    # Cálculo Gi* (USANDO VALORES ABSOLUTOS, PADRÃO ARCGIS)
    gi = G_Local(valores, w_final, star=None)
    z_scores = np.nan_to_num(gi.Zs, nan=0.0)
    p_raw = np.nan_to_num(gi.p_sim, nan=1.0)

    # Correção FDR
    reject, p_fdr, _, _ = multipletests(p_raw, alpha=0.10, method='fdr_bh')

    # Classificação
    gi_bins = [classificar_gi_bin(z, p) for z, p in zip(z_scores, p_fdr)]

    # Armazenamento Unificado
    df_master[f'CASOS_{ano}'] = valores
    df_master[f'Gi_ZScore_{ano}'] = np.round(z_scores, 4)
    df_master[f'Gi_P_FDR_{ano}'] = np.round(p_fdr, 5)
    df_master[f'Gi_Bin_{ano}'] = gi_bins
;
# 5. CÁLCULO DE TOTAIS
print("\n[3/4] Calculando Totais...")

cols_bins = [c for c in df_master.columns if 'Gi_Bin_' in c]

# Nomes sem espaços (JavaScript friendly)
df_master['TOTAL_Gi_Bin_Positivo'] = df_master[cols_bins].map(lambda x: x if x > 0 else 0).sum(axis=1)
df_master['TOTAL_Recorrencia'] = (df_master[cols_bins] > 0).astype(int).sum(axis=1)

# 6. EXPORTAÇÃO
print("\n[4/4] Exportando Resultados...")

path_gpkg = os.path.join(pasta_saida, "resultado_estratificacao_4B.gpkg")
path_xlsx = os.path.join(pasta_saida, "relatorio_analitico_4B.xlsx")

# Excel Unificado
df_excel = pd.DataFrame(df_master.drop(columns='geometry'))
df_excel.to_excel(path_xlsx, sheet_name="DADOS_UNIFICADOS", index=False)

# GPKG
for ano in colunas_anos:
    df_master.to_file(path_gpkg, layer=f"Gi_Bin_{ano}", driver="GPKG")
df_master.to_file(path_gpkg, layer="TOTAL_Gi_Bin_Positivo", driver="GPKG")
df_master.to_file(path_gpkg, layer="TOTAL_Recorrencia", driver="GPKG")

# 7. GERAÇÃO DO MAPA HTML
print("      > Gerando Mapa Interativo...")

df_mapa = df_master.to_crs(epsg=4326)

# Converte colunas críticas para int/float nativos
for col in ['TOTAL_Recorrencia', 'TOTAL_Gi_Bin_Positivo']:
    if col in df_mapa.columns:
        df_mapa[col] = pd.to_numeric(df_mapa[col]).fillna(0)
        if 'Recorrencia' in col:
            df_mapa[col] = df_mapa[col].astype(int)
        else:
            df_mapa[col] = df_mapa[col].astype(float)

for col in cols_bins:
    if col in df_mapa.columns:
        df_mapa[col] = pd.to_numeric(df_mapa[col]).fillna(0).astype(int)

# Centroide
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    centro_temp = gdf_analise.to_crs(gdf_analise.estimate_utm_crs()).geometry.centroid.to_crs(epsg=4326)
    y_cent = centro_temp.y.mean()
    x_cent = centro_temp.x.mean()

m = folium.Map(location=[y_cent, x_cent], zoom_start=13, tiles='CartoDB positron')

# --- ESTILOS DE CORES ---

def get_style_ano(feature, col_alvo):
    val = feature['properties'][col_alvo]
    colors = {
        3: '#d73027', 2: '#f46d43', 1: '#fdae61',
        0: '#f7f7f7',
        -1: '#abd9e9', -2: '#74add1', -3: '#4575b4'
    }
    fill = colors.get(val, '#f7f7f7')
    opacity = 0 if val == 0 else 0.7
    return {'fillColor': fill, 'fillOpacity': opacity, 'weight': 1, 'color': 'black', 'opacity': 0.5}

def get_style_positivo(feature, col_alvo, max_val):
    val = feature['properties'][col_alvo]
    if val <= 0:
        return {'fillOpacity': 0, 'weight': 1, 'color': 'black', 'opacity': 0.5}

    intensidade = val / max(max_val, 1)
    if intensidade <= 0.2: color = '#fee5d9'
    elif intensidade <= 0.4: color = '#fcae91'
    elif intensidade <= 0.6: color = '#fb6a4a'
    elif intensidade <= 0.8: color = '#de2d26'
    else: color = '#a50f15'

    return {'fillColor': color, 'fillOpacity': 0.7, 'weight': 1, 'color': 'black', 'opacity': 0.5}

def get_style_recorrencia(feature, col_alvo):
    val = feature['properties'][col_alvo]
    if val == 0:
        return {'fillOpacity': 0, 'weight': 1, 'color': 'black', 'opacity': 0.5}

    if val == 1: color = '#FA8072'
    elif val <= 3: color = '#FF0000'
    elif val <= 5: color = '#B22222'
    else: color = '#8B0000'
    return {'fillColor': color, 'fillOpacity': 0.8, 'weight': 1, 'color': 'black', 'opacity': 0.5}

# --- ADIÇÃO DAS CAMADAS ---

folium.GeoJson(
    df_mapa,
    name='TOTAL Recorrencia',
    style_function=lambda feature: get_style_recorrencia(feature, 'TOTAL_Recorrencia'),
    tooltip=folium.GeoJsonTooltip(fields=[col_id, 'TOTAL_Recorrencia'], aliases=['Local:', 'Anos:'])
).add_to(m)

max_positivo = df_master['TOTAL_Gi_Bin_Positivo'].max()
folium.GeoJson(
    df_mapa,
    name='TOTAL Gi_Bin Positivo',
    style_function=lambda feature: get_style_positivo(feature, 'TOTAL_Gi_Bin_Positivo', max_positivo),
    tooltip=folium.GeoJsonTooltip(fields=[col_id, 'TOTAL_Gi_Bin_Positivo'], aliases=['Local:', 'Score Total:']),
    show=False
).add_to(m)

for ano in colunas_anos:
    col_alvo = f'Gi_Bin_{ano}'
    folium.GeoJson(
        df_mapa,
        name=f'Gi_Bin {ano}',
        style_function=lambda feature, c=col_alvo: get_style_ano(feature, c),
        tooltip=folium.GeoJsonTooltip(fields=[col_id, col_alvo], aliases=['Local:', f'Gi_Bin {ano}:']),
        show=False
    ).add_to(m)

folium.LayerControl().add_to(m)

total_anos_count = len(colunas_anos)
legend_html = f'''
<div style="position: fixed; bottom: 30px; left: 30px; z-index:9999; font-size:12px;
            background-color:white; padding: 10px; border: 1px solid grey; border-radius: 5px;">
  <b>Legenda (Recorrência)</b><br>
  <i style="border:1px solid black; width:15px; height:15px; display:inline-block;"></i> 0 Anos (Transp.)<br>
  <i style="background:#FA8072; width:15px; height:15px; display:inline-block;"></i> 1 Ano<br>
  <i style="background:#FF0000; width:15px; height:15px; display:inline-block;"></i> 2 ou 3 Anos<br>
  <i style="background:#B22222; width:15px; height:15px; display:inline-block;"></i> 4 ou 5 Anos<br>
  <i style="background:#8B0000; width:15px; height:15px; display:inline-block;"></i> 6 Anos ou mais<br>
  <br>Total analisado: {total_anos_count} anos<br>
  <hr style="margin:5px 0;">
  <b>Legenda (Gi_Bin Anual)</b><br>
  <span style="color:#d73027;">■</span> Hotspot (+1 a +3)<br>
  <span style="color:#4575b4;">■</span> Coldspot (-1 a -3)
</div>
'''
m.get_root().html.add_child(folium.Element(legend_html))

m.save(os.path.join(pasta_saida, "MAPA_INTERATIVO_4B.html"))

print("\n" + "="*60)
print("PROCESSO 4.B CONCLUÍDO (PADRÃO ARCGIS - BLINDADO)")
print("="*60)
print(f"Arquivos gerados em: '{pasta_saida}'")
display(m)

"""# BACKUP FINAL E DOWNLOAD"""

print("\n" + "="*60)
print(">>> FINALIZANDO: GERANDO BACKUP COMPLETO DO AMBIENTE")
print("="*60)

# Define nome com data/hora para organização
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
nome_zip = f"BACKUP_PROJETO_{timestamp}.zip"

print(f"    > Compactando pastas (cache, verificacao, resultados)...")

# Comando de sistema (Linux) para compactação
# -r: recursivo (pastas inteiras)
# -q: silencioso (quiet)
# -x: EXCLUI o drive montado (CRÍTICO) e arquivos de sistema do Colab
comando_zip = f"zip -r -q {nome_zip} . -x 'drive/*' 'sample_data/*' '.config/*'"
os.system(comando_zip)

# Verifica e inicia o download
if os.path.exists(nome_zip):
    tamanho_mb = os.path.getsize(nome_zip) / (1024 * 1024)
    print(f"    > Arquivo gerado: {nome_zip} ({tamanho_mb:.2f} MB)")
    print("    > Disparando download para sua máquina local...")

    try:
        files.download(nome_zip)
    except Exception as e:
        print(f"    [AVISO] O download automático falhou ({e}).")
        print("            Localize o arquivo na aba lateral esquerda e baixe manualmente.")
else:
    print("    [ERRO] Falha na criação do arquivo ZIP.")

print("-" * 60)
print("ROTINA ENCERRADA.")